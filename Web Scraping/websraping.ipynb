{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43aeae72-037b-4b37-9b70-24a714f12eb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moderator Elections 2025 -> https://www.diabetes.co.uk/forum/threads/moderator-elections-2025.209890/\n",
      "Diabetes Burnout. -> https://www.diabetes.co.uk/forum/threads/diabetes-burnout.13994/\n",
      "It doesn't have to turn out like that Panorama programme suggested -> https://www.diabetes.co.uk/forum/threads/it-doesnt-have-to-turn-out-like-that-panorama-programme-suggested.108923/\n",
      "Crowdstrike killed the NHS app -> https://www.diabetes.co.uk/forum/threads/crowdstrike-killed-the-nhs-app.205309/\n",
      "Librelink IOS issue info, â€œwhite screen of death.â€ ; How to resolve. -> https://www.diabetes.co.uk/forum/threads/librelink-ios-issue-info-%E2%80%9Cwhite-screen-of-death-%E2%80%9D-how-to-resolve.197946/\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "\n",
    "def init_driver(headless=True):\n",
    "    options = webdriver.ChromeOptions()\n",
    "    if headless:\n",
    "        options.add_argument(\"--headless\")\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    return driver\n",
    "\n",
    "def get_threads_in_category(driver, cat_url, max_threads=10):\n",
    "    driver.get(cat_url)\n",
    "    wait = WebDriverWait(driver, 20)  # wait longer just in case\n",
    "    \n",
    "    # Correct XenForo thread title selector\n",
    "    thread_links = wait.until(\n",
    "        EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"div.structItem-title a\"))\n",
    "    )\n",
    "    \n",
    "    threads = []\n",
    "    for t in thread_links[:max_threads]:\n",
    "        href = t.get_attribute(\"href\")\n",
    "        title = t.text.strip()\n",
    "        threads.append((title, href))\n",
    "    return threads\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    driver = init_driver(headless=False)\n",
    "    category_url = \"https://www.diabetes.co.uk/forum/category/diabetes-discussions.1/\"\n",
    "    \n",
    "    threads = get_threads_in_category(driver, category_url, max_threads=5)\n",
    "    for title, link in threads:\n",
    "        print(title, \"->\", link)\n",
    "    \n",
    "    driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68d8abe4-ab2d-47e1-87e0-168db6862a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html id=\"XF\" lang=\"en-GB\" dir=\"LTR\" data-xf=\"2.3\" data-app=\"public\" data-template=\"forum_view\" data-container-key=\"node-1\" data-content-key=\"forum-1\" data-logged-in=\"false\" data-cookie-prefix=\"xf_\" data-csrf=\"1759433781,ce432d05d6194eb7631215a0f7c28da5\" class=\"has-js template-forum_view has-no-touchevents has-passiveeventlisteners has-no-hiddenscroll has-overflowanchor has-no-displaymodestandalone has-flexgap has-os-windows has-browser-chrome\"><head>\n",
      "\t\n",
      "\t\t<link rel=\"amphtml\" href=\"https://www.diabetes.co.uk/forum/category/diabetes-discussions.1/?amp=1\">\n",
      "\t\n",
      "\n",
      "\t\n",
      "\t\n",
      "\t\n",
      "\n",
      "\t<meta charset=\"utf-8\">\n",
      "\t<title>Diabetes Discussions | Diabetes Forum</title>\n",
      "\t<link rel=\"manifest\" href=\"/forum/webmanifest.php\">\n",
      "\n",
      "\t<meta http-equiv=\"X-UA-Compatible\" content=\"IE=Edge\">\n",
      "\t<meta name=\"viewport\" content=\"width=device-width, initial-scale=1, viewport-fit=cover\">\n",
      "\n",
      "\t\n",
      "\t\t<meta name=\"theme-color\" content=\"#0f578a\">\n",
      "\t\n",
      "\n",
      "\t<meta name=\"apple-mobile-web-app-title\" content=\"DCUK\">\n",
      "\t\n",
      "\t\t<link rel=\"apple-touch-icon\" href=\"/foru\n"
     ]
    }
   ],
   "source": [
    "driver.get(\"https://www.diabetes.co.uk/forum/category/diabetes-discussions.1/\")\n",
    "print(driver.page_source[:1000])  # preview HTML\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dcfe6636-016e-43ef-a968-3774f96bc4d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 threads\n",
      "\n",
      "Scraping: Moderator Elections 2025 -> https://www.diabetes.co.uk/forum/threads/moderator-elections-2025.209890/\n",
      "\n",
      "Scraping: Diabetes Burnout. -> https://www.diabetes.co.uk/forum/threads/diabetes-burnout.13994/\n",
      "\n",
      "Scraping: It doesn't have to turn out like that Panorama programme suggested -> https://www.diabetes.co.uk/forum/threads/it-doesnt-have-to-turn-out-like-that-panorama-programme-suggested.108923/\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import csv\n",
    "\n",
    "def init_driver(headless=True):\n",
    "    options = webdriver.ChromeOptions()\n",
    "    if headless:\n",
    "        options.add_argument(\"--headless=new\")\n",
    "    options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    return webdriver.Chrome(options=options)\n",
    "\n",
    "def get_threads_in_category(driver, cat_url, max_threads=5):\n",
    "    driver.get(cat_url)\n",
    "    wait = WebDriverWait(driver, 20)\n",
    "    thread_links = wait.until(\n",
    "        EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"div.structItem-title a\"))\n",
    "    )\n",
    "    threads = []\n",
    "    for t in thread_links[:max_threads]:\n",
    "        threads.append((t.text.strip(), t.get_attribute(\"href\")))\n",
    "    return threads\n",
    "\n",
    "def scrape_thread(driver, thread_url):\n",
    "    driver.get(thread_url)\n",
    "    wait = WebDriverWait(driver, 20)\n",
    "\n",
    "    posts = []\n",
    "    while True:\n",
    "        post_elems = wait.until(\n",
    "            EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"div.bbWrapper\"))\n",
    "        )\n",
    "        for p in post_elems:\n",
    "            text = p.text.strip()\n",
    "            if text:\n",
    "                posts.append(text)\n",
    "\n",
    "        # check if there is a \"next page\" button\n",
    "        try:\n",
    "            next_btn = driver.find_element(By.CSS_SELECTOR, \"a.pageNav-jump--next\")\n",
    "            next_url = next_btn.get_attribute(\"href\")\n",
    "            if next_url:\n",
    "                driver.get(next_url)\n",
    "                time.sleep(2)\n",
    "                continue\n",
    "        except:\n",
    "            break  # no more pages\n",
    "\n",
    "    return posts\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    driver = init_driver(headless=False)\n",
    "    category_url = \"https://www.diabetes.co.uk/forum/category/diabetes-discussions.1/\"\n",
    "    \n",
    "    # get some threads\n",
    "    threads = get_threads_in_category(driver, category_url, max_threads=3)\n",
    "    print(f\"Found {len(threads)} threads\")\n",
    "\n",
    "    # open CSV\n",
    "    with open(\"diabetes_forum_posts.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"Thread Title\", \"Thread URL\", \"Post Index\", \"Post Text\"])\n",
    "\n",
    "        for title, link in threads:\n",
    "            print(f\"\\nScraping: {title} -> {link}\")\n",
    "            posts = scrape_thread(driver, link)\n",
    "            for i, post in enumerate(posts, start=1):\n",
    "                writer.writerow([title, link, i, post])\n",
    "\n",
    "    driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "000b387f-4069-4bb1-8799-c895d9e18d5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 threads\n",
      "\n",
      "Scraping: Moderator Elections 2025 -> https://www.diabetes.co.uk/forum/threads/moderator-elections-2025.209890/\n",
      "Saved: diabetes_forum_texts\\Moderator_Elections_2025.txt\n",
      "\n",
      "Scraping: Diabetes Burnout. -> https://www.diabetes.co.uk/forum/threads/diabetes-burnout.13994/\n",
      "Saved: diabetes_forum_texts\\Diabetes_Burnout_.txt\n",
      "\n",
      "Scraping: It doesn't have to turn out like that Panorama programme suggested -> https://www.diabetes.co.uk/forum/threads/it-doesnt-have-to-turn-out-like-that-panorama-programme-suggested.108923/\n",
      "Saved: diabetes_forum_texts\\It_doesn_t_have_to_turn_out_like_that_Panorama_pro.txt\n",
      "\n",
      "All threads saved in 'diabetes_forum_texts/' folder\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "\n",
    "def init_driver(headless=True):\n",
    "    options = webdriver.ChromeOptions()\n",
    "    if headless:\n",
    "        options.add_argument(\"--headless=new\")\n",
    "    options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    return webdriver.Chrome(options=options)\n",
    "\n",
    "def get_threads_in_category(driver, cat_url, max_threads=5):\n",
    "    driver.get(cat_url)\n",
    "    wait = WebDriverWait(driver, 20)\n",
    "    thread_links = wait.until(\n",
    "        EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"div.structItem-title a\"))\n",
    "    )\n",
    "    threads = []\n",
    "    for t in thread_links[:max_threads]:\n",
    "        threads.append((t.text.strip(), t.get_attribute(\"href\")))\n",
    "    return threads\n",
    "\n",
    "def scrape_thread(driver, thread_url):\n",
    "    driver.get(thread_url)\n",
    "    wait = WebDriverWait(driver, 20)\n",
    "\n",
    "    posts = []\n",
    "    while True:\n",
    "        post_elems = wait.until(\n",
    "            EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"div.bbWrapper\"))\n",
    "        )\n",
    "        for p in post_elems:\n",
    "            text = p.text.strip()\n",
    "            if text:\n",
    "                posts.append(text)\n",
    "\n",
    "        # next page check\n",
    "        try:\n",
    "            next_btn = driver.find_element(By.CSS_SELECTOR, \"a.pageNav-jump--next\")\n",
    "            next_url = next_btn.get_attribute(\"href\")\n",
    "            if next_url:\n",
    "                driver.get(next_url)\n",
    "                time.sleep(2)\n",
    "                continue\n",
    "        except:\n",
    "            break\n",
    "    return posts\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    driver = init_driver(headless=False)\n",
    "    category_url = \"https://www.diabetes.co.uk/forum/category/diabetes-discussions.1/\"\n",
    "    \n",
    "    threads = get_threads_in_category(driver, category_url, max_threads=3)\n",
    "    print(f\"Found {len(threads)} threads\")\n",
    "\n",
    "    # Make folder for saving text files\n",
    "    os.makedirs(\"diabetes_forum_texts\", exist_ok=True)\n",
    "\n",
    "    for title, link in threads:\n",
    "        print(f\"\\nScraping: {title} -> {link}\")\n",
    "        posts = scrape_thread(driver, link)\n",
    "\n",
    "        # Clean filename (remove illegal chars)\n",
    "        safe_title = \"\".join(c if c.isalnum() else \"_\" for c in title)[:50]\n",
    "        file_path = os.path.join(\"diabetes_forum_texts\", f\"{safe_title}.txt\")\n",
    "\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(f\"Thread: {title}\\nURL: {link}\\n\\n\")\n",
    "            for i, post in enumerate(posts, start=1):\n",
    "                f.write(f\"Post {i}:\\n{post}\\n\\n{'-'*40}\\n\\n\")\n",
    "\n",
    "        print(f\"Saved: {file_path}\")\n",
    "\n",
    "    driver.quit()\n",
    "    print(\"\\nAll threads saved in 'diabetes_forum_texts/' folder\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1125825-c720-440d-8c6e-ad8b2b176a99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25 threads\n",
      "\n",
      "Scraping: Moderator Elections 2025 -> https://www.diabetes.co.uk/forum/threads/moderator-elections-2025.209890/\n",
      "Saved: diabetes_forum_texts\\Moderator_Elections_2025.txt\n",
      "\n",
      "Scraping: Diabetes Burnout. -> https://www.diabetes.co.uk/forum/threads/diabetes-burnout.13994/\n",
      "Saved: diabetes_forum_texts\\Diabetes_Burnout_.txt\n",
      "\n",
      "Scraping: It doesn't have to turn out like that Panorama programme suggested -> https://www.diabetes.co.uk/forum/threads/it-doesnt-have-to-turn-out-like-that-panorama-programme-suggested.108923/\n",
      "Saved: diabetes_forum_texts\\It_doesn_t_have_to_turn_out_like_that_Panorama_pro.txt\n",
      "\n",
      "Scraping: Crowdstrike killed the NHS app -> https://www.diabetes.co.uk/forum/threads/crowdstrike-killed-the-nhs-app.205309/\n",
      "Saved: diabetes_forum_texts\\Crowdstrike_killed_the_NHS_app.txt\n",
      "\n",
      "Scraping: Librelink IOS issue info, â€œwhite screen of death.â€ ; How to resolve. -> https://www.diabetes.co.uk/forum/threads/librelink-ios-issue-info-%E2%80%9Cwhite-screen-of-death-%E2%80%9D-how-to-resolve.197946/\n",
      "Saved: diabetes_forum_texts\\Librelink_IOS_issue_info___white_screen_of_death__.txt\n",
      "\n",
      "Scraping: Test Our Apps Before Anyone Else - Become A Beta Tester! -> https://www.diabetes.co.uk/forum/threads/test-our-apps-before-anyone-else-become-a-beta-tester.156225/\n",
      "Saved: diabetes_forum_texts\\Test_Our_Apps_Before_Anyone_Else___Become_A_Beta_T.txt\n",
      "\n",
      "Scraping: What was your fasting blood glucose? (with some chat) -> https://www.diabetes.co.uk/forum/threads/what-was-your-fasting-blood-glucose-with-some-chat.22272/\n",
      "Saved: diabetes_forum_texts\\What_was_your_fasting_blood_glucose___with_some_ch.txt\n",
      "\n",
      "Scraping: Hi Iâ€™m julieclarke Iâ€™ve joined to find out more about T2 diabetes -> https://www.diabetes.co.uk/forum/threads/hi-i%E2%80%99m-julieclarke-i%E2%80%99ve-joined-to-find-out-more-about-t2-diabetes.210409/\n",
      "Saved: diabetes_forum_texts\\Hi_I_m_julieclarke_I_ve_joined_to_find_out_more_ab.txt\n",
      "\n",
      "Scraping: Chocolate -> https://www.diabetes.co.uk/forum/threads/chocolate.210062/\n",
      "Saved: diabetes_forum_texts\\Chocolate.txt\n",
      "\n",
      "Scraping: Anyone on the Ascend Plus trial? -> https://www.diabetes.co.uk/forum/threads/anyone-on-the-ascend-plus-trial.199683/\n",
      "Saved: diabetes_forum_texts\\Anyone_on_the_Ascend_Plus_trial_.txt\n",
      "\n",
      "Scraping: Dry throat. -> https://www.diabetes.co.uk/forum/threads/dry-throat.207576/\n",
      "Saved: diabetes_forum_texts\\Dry_throat_.txt\n",
      "\n",
      "Scraping: To metformin or not metformin, that is the question -> https://www.diabetes.co.uk/forum/threads/to-metformin-or-not-metformin-that-is-the-question.209038/\n",
      "Saved: diabetes_forum_texts\\To_metformin_or_not_metformin__that_is_the_questio.txt\n",
      "\n",
      "Scraping: Gliclazide -> https://www.diabetes.co.uk/forum/threads/gliclazide.9940/\n",
      "Saved: diabetes_forum_texts\\Gliclazide.txt\n",
      "\n",
      "Scraping: \"the time has come\" the walrus said \"to talk of many things...\" -> https://www.diabetes.co.uk/forum/threads/the-time-has-come-the-walrus-said-to-talk-of-many-things.210119/\n",
      "Saved: diabetes_forum_texts\\_the_time_has_come__the_walrus_said__to_talk_of_ma.txt\n",
      "\n",
      "Scraping: Semaglutide -> https://www.diabetes.co.uk/forum/threads/semaglutide.203694/\n",
      "Saved: diabetes_forum_texts\\Semaglutide.txt\n",
      "\n",
      "Scraping: Quadrophenia. -> https://www.diabetes.co.uk/forum/threads/quadrophenia.210222/\n",
      "Saved: diabetes_forum_texts\\Quadrophenia_.txt\n",
      "\n",
      "Scraping: Changing clinics -> https://www.diabetes.co.uk/forum/threads/changing-clinics.210321/\n",
      "Saved: diabetes_forum_texts\\Changing_clinics.txt\n",
      "\n",
      "Scraping: Insulin not working to bring sugar levels down!? -> https://www.diabetes.co.uk/forum/threads/insulin-not-working-to-bring-sugar-levels-down.37694/\n",
      "Saved: diabetes_forum_texts\\Insulin_not_working_to_bring_sugar_levels_down__.txt\n",
      "\n",
      "Scraping: Diabetic Backpack -> https://www.diabetes.co.uk/forum/threads/diabetic-backpack.210314/\n",
      "Saved: diabetes_forum_texts\\Diabetic_Backpack.txt\n",
      "\n",
      "Scraping: Low potassium options -> https://www.diabetes.co.uk/forum/threads/low-potassium-options.210299/\n",
      "Saved: diabetes_forum_texts\\Low_potassium_options.txt\n",
      "\n",
      "Scraping: Libre 2 -> https://www.diabetes.co.uk/forum/threads/libre-2.210277/\n",
      "Saved: diabetes_forum_texts\\Libre_2.txt\n",
      "\n",
      "Scraping: Dapagliflozin -> https://www.diabetes.co.uk/forum/threads/dapagliflozin.182997/\n",
      "Saved: diabetes_forum_texts\\Dapagliflozin.txt\n",
      "\n",
      "Scraping: Ascend trial -> https://www.diabetes.co.uk/forum/threads/ascend-trial.210289/\n",
      "Saved: diabetes_forum_texts\\Ascend_trial.txt\n",
      "\n",
      "Scraping: Slightly confused -> https://www.diabetes.co.uk/forum/threads/slightly-confused.210247/\n",
      "Saved: diabetes_forum_texts\\Slightly_confused.txt\n",
      "\n",
      "Scraping: FreeStyle Libre 14 day -> https://www.diabetes.co.uk/forum/threads/freestyle-libre-14-day.210265/\n",
      "Saved: diabetes_forum_texts\\FreeStyle_Libre_14_day.txt\n",
      "\n",
      "All threads saved in 'diabetes_forum_texts/' folder\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "def init_driver(headless=True):\n",
    "    options = webdriver.ChromeOptions()\n",
    "    if headless:\n",
    "        options.add_argument(\"--headless=new\")\n",
    "    options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    return webdriver.Chrome(options=options)\n",
    "\n",
    "def get_threads_in_category(driver, cat_url, max_threads=25):\n",
    "    driver.get(cat_url)\n",
    "    wait = WebDriverWait(driver, 20)\n",
    "\n",
    "    threads = []\n",
    "    seen = set()\n",
    "\n",
    "    while len(threads) < max_threads:\n",
    "        # Wait for thread titles to load\n",
    "        thread_links = wait.until(\n",
    "            EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"div.structItem-title a\"))\n",
    "        )\n",
    "\n",
    "        for t in thread_links:\n",
    "            href = t.get_attribute(\"href\")\n",
    "            title = t.text.strip()\n",
    "            if href and href not in seen:\n",
    "                threads.append((title, href))\n",
    "                seen.add(href)\n",
    "                if len(threads) >= max_threads:\n",
    "                    break\n",
    "\n",
    "        # Scroll to bottom to load more threads\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(2)  # allow time for new threads to load\n",
    "\n",
    "    return threads\n",
    "\n",
    "def scrape_thread(driver, thread_url):\n",
    "    driver.get(thread_url)\n",
    "    wait = WebDriverWait(driver, 20)\n",
    "\n",
    "    posts = []\n",
    "    while True:\n",
    "        post_elems = wait.until(\n",
    "            EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"div.bbWrapper\"))\n",
    "        )\n",
    "        for p in post_elems:\n",
    "            text = p.text.strip()\n",
    "            if text:\n",
    "                posts.append(text)\n",
    "\n",
    "        # Next page check\n",
    "        try:\n",
    "            next_btn = driver.find_element(By.CSS_SELECTOR, \"a.pageNav-jump--next\")\n",
    "            next_url = next_btn.get_attribute(\"href\")\n",
    "            if next_url:\n",
    "                driver.get(next_url)\n",
    "                time.sleep(2)\n",
    "                continue\n",
    "        except:\n",
    "            break\n",
    "    return posts\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    driver = init_driver(headless=False)\n",
    "    category_url = \"https://www.diabetes.co.uk/forum/category/diabetes-discussions.1/\"\n",
    "    \n",
    "    threads = get_threads_in_category(driver, category_url, max_threads=25)\n",
    "    print(f\"Found {len(threads)} threads\")\n",
    "\n",
    "    # Folder for saving\n",
    "    os.makedirs(\"diabetes_forum_texts\", exist_ok=True)\n",
    "\n",
    "    for title, link in threads:\n",
    "        print(f\"\\nScraping: {title} -> {link}\")\n",
    "        posts = scrape_thread(driver, link)\n",
    "\n",
    "        # Clean filename\n",
    "        safe_title = \"\".join(c if c.isalnum() else \"_\" for c in title)[:50]\n",
    "        file_path = os.path.join(\"diabetes_forum_texts\", f\"{safe_title}.txt\")\n",
    "\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(f\"Thread: {title}\\nURL: {link}\\n\\n\")\n",
    "            for i, post in enumerate(posts, start=1):\n",
    "                f.write(f\"Post {i}:\\n{post}\\n\\n{'-'*40}\\n\\n\")\n",
    "\n",
    "        print(f\"Saved: {file_path}\")\n",
    "\n",
    "    driver.quit()\n",
    "    print(\"\\nAll threads saved in 'diabetes_forum_texts/' folder\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea11777a-d676-4dc0-8e56-119a1b2b1c8e",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 130\u001b[0m\n\u001b[0;32m    127\u001b[0m         driver\u001b[38;5;241m.\u001b[39mquit()\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 130\u001b[0m     scrape_all_latest(target_topics\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m)\n",
      "Cell \u001b[1;32mIn[3], line 121\u001b[0m, in \u001b[0;36mscrape_all_latest\u001b[1;34m(target_topics)\u001b[0m\n\u001b[0;32m    119\u001b[0m driver \u001b[38;5;241m=\u001b[39m init_driver(headless\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)  \u001b[38;5;66;03m# Set headless=True to run without UI\u001b[39;00m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 121\u001b[0m     topic_links \u001b[38;5;241m=\u001b[39m scroll_and_collect_topics(driver, target_count\u001b[38;5;241m=\u001b[39mtarget_topics)\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m idx, (title, href) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(topic_links, start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m    123\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mScraping topic \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(topic_links)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtitle\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[3], line 20\u001b[0m, in \u001b[0;36mscroll_and_collect_topics\u001b[1;34m(driver, target_count, pause_time)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mscroll_and_collect_topics\u001b[39m(driver, target_count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m, pause_time\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m):\n\u001b[0;32m     19\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Scrolls the latest page to collect topic links.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m     driver\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://community.patient.info/c/heart-health-and-blood-vessels/18\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     21\u001b[0m     topic_links \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(topic_links) \u001b[38;5;241m<\u001b[39m target_count:\n\u001b[0;32m     24\u001b[0m         \u001b[38;5;66;03m# Scroll to bottom\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:483\u001b[0m, in \u001b[0;36mWebDriver.get\u001b[1;34m(self, url)\u001b[0m\n\u001b[0;32m    465\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, url: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    466\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Navigate the browser to the specified URL in the current window or\u001b[39;00m\n\u001b[0;32m    467\u001b[0m \u001b[38;5;124;03m    tab.\u001b[39;00m\n\u001b[0;32m    468\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    481\u001b[0m \u001b[38;5;124;03m    >>> driver.get(\"https://example.com\")\u001b[39;00m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 483\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecute(Command\u001b[38;5;241m.\u001b[39mGET, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m\"\u001b[39m: url})\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:455\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    452\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msessionId\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m params:\n\u001b[0;32m    453\u001b[0m         params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msessionId\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession_id\n\u001b[1;32m--> 455\u001b[0m response \u001b[38;5;241m=\u001b[39m cast(RemoteConnection, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_executor)\u001b[38;5;241m.\u001b[39mexecute(driver_command, params)\n\u001b[0;32m    457\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[0;32m    458\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_handler\u001b[38;5;241m.\u001b[39mcheck_response(response)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\remote\\remote_connection.py:405\u001b[0m, in \u001b[0;36mRemoteConnection.execute\u001b[1;34m(self, command, params)\u001b[0m\n\u001b[0;32m    403\u001b[0m trimmed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trim_large_entries(params)\n\u001b[0;32m    404\u001b[0m LOGGER\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, command_info[\u001b[38;5;241m0\u001b[39m], url, \u001b[38;5;28mstr\u001b[39m(trimmed))\n\u001b[1;32m--> 405\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(command_info[\u001b[38;5;241m0\u001b[39m], url, body\u001b[38;5;241m=\u001b[39mdata)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\remote\\remote_connection.py:429\u001b[0m, in \u001b[0;36mRemoteConnection._request\u001b[1;34m(self, method, url, body)\u001b[0m\n\u001b[0;32m    426\u001b[0m     body \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    428\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client_config\u001b[38;5;241m.\u001b[39mkeep_alive:\n\u001b[1;32m--> 429\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conn\u001b[38;5;241m.\u001b[39mrequest(method, url, body\u001b[38;5;241m=\u001b[39mbody, headers\u001b[38;5;241m=\u001b[39mheaders, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client_config\u001b[38;5;241m.\u001b[39mtimeout)\n\u001b[0;32m    430\u001b[0m     statuscode \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mstatus\n\u001b[0;32m    431\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\_request_methods.py:143\u001b[0m, in \u001b[0;36mRequestMethods.request\u001b[1;34m(self, method, url, body, fields, headers, json, **urlopen_kw)\u001b[0m\n\u001b[0;32m    135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_encode_url(\n\u001b[0;32m    136\u001b[0m         method,\n\u001b[0;32m    137\u001b[0m         url,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39murlopen_kw,\n\u001b[0;32m    141\u001b[0m     )\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_encode_body(\n\u001b[0;32m    144\u001b[0m         method, url, fields\u001b[38;5;241m=\u001b[39mfields, headers\u001b[38;5;241m=\u001b[39mheaders, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39murlopen_kw\n\u001b[0;32m    145\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\_request_methods.py:278\u001b[0m, in \u001b[0;36mRequestMethods.request_encode_body\u001b[1;34m(self, method, url, fields, headers, encode_multipart, multipart_boundary, **urlopen_kw)\u001b[0m\n\u001b[0;32m    274\u001b[0m     extra_kw[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Type\u001b[39m\u001b[38;5;124m\"\u001b[39m, content_type)\n\u001b[0;32m    276\u001b[0m extra_kw\u001b[38;5;241m.\u001b[39mupdate(urlopen_kw)\n\u001b[1;32m--> 278\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murlopen(method, url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mextra_kw)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\poolmanager.py:459\u001b[0m, in \u001b[0;36mPoolManager.urlopen\u001b[1;34m(self, method, url, redirect, **kw)\u001b[0m\n\u001b[0;32m    457\u001b[0m     response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(method, url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    458\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 459\u001b[0m     response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(method, u\u001b[38;5;241m.\u001b[39mrequest_uri, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    461\u001b[0m redirect_location \u001b[38;5;241m=\u001b[39m redirect \u001b[38;5;129;01mand\u001b[39;00m response\u001b[38;5;241m.\u001b[39mget_redirect_location()\n\u001b[0;32m    462\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m redirect_location:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    784\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    786\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 787\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[0;32m    788\u001b[0m     conn,\n\u001b[0;32m    789\u001b[0m     method,\n\u001b[0;32m    790\u001b[0m     url,\n\u001b[0;32m    791\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[0;32m    792\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[0;32m    793\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    794\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    795\u001b[0m     retries\u001b[38;5;241m=\u001b[39mretries,\n\u001b[0;32m    796\u001b[0m     response_conn\u001b[38;5;241m=\u001b[39mresponse_conn,\n\u001b[0;32m    797\u001b[0m     preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[0;32m    798\u001b[0m     decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[0;32m    799\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw,\n\u001b[0;32m    800\u001b[0m )\n\u001b[0;32m    802\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[0;32m    803\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:534\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    532\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[0;32m    533\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 534\u001b[0m     response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[0;32m    535\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    536\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connection.py:565\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    562\u001b[0m _shutdown \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshutdown\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    564\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[1;32m--> 565\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[0;32m    567\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    568\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\http\\client.py:1428\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1426\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1427\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1428\u001b[0m         response\u001b[38;5;241m.\u001b[39mbegin()\n\u001b[0;32m   1429\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[0;32m   1430\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\http\\client.py:331\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    329\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[0;32m    330\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 331\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_status()\n\u001b[0;32m    332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[0;32m    333\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\http\\client.py:292\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 292\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mreadline(_MAXLINE \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    293\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[0;32m    294\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\socket.py:708\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    706\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    707\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 708\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39mrecv_into(b)\n\u001b[0;32m    709\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    710\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "def init_driver(headless=True):\n",
    "    options = webdriver.ChromeOptions()\n",
    "    if headless:\n",
    "        options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument(\"--window-size=1920,1080\")\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    driver.implicitly_wait(5)\n",
    "    return driver\n",
    "\n",
    "def scroll_and_collect_topics(driver, target_count=200, pause_time=2):\n",
    "    \"\"\"Scrolls the latest page to collect topic links.\"\"\"\n",
    "    driver.get(\"https://community.patient.info/c/heart-health-and-blood-vessels/18\")\n",
    "    topic_links = []\n",
    "\n",
    "    while len(topic_links) < target_count:\n",
    "        # Scroll to bottom\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(pause_time)\n",
    "\n",
    "        # Collect links\n",
    "        topics = driver.find_elements(By.CSS_SELECTOR, \"a.title.raw-topic-link\")\n",
    "        for topic in topics:\n",
    "            href = topic.get_attribute(\"href\")\n",
    "            title = topic.text.strip()\n",
    "            if href and (title, href) not in topic_links:\n",
    "                topic_links.append((title, href))\n",
    "\n",
    "    print(f\"Collected {len(topic_links[:target_count])} topics\")\n",
    "    return topic_links[:target_count]\n",
    "\n",
    "def scroll_to_load_posts(driver, target_count=9999, pause_time=1):\n",
    "    \"\"\"Scrolls inside a topic to load all posts.\"\"\"\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    posts_loaded = len(driver.find_elements(By.CSS_SELECTOR, \"div.topic-post\"))\n",
    "\n",
    "    while posts_loaded < target_count:\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(pause_time)\n",
    "        posts_loaded = len(driver.find_elements(By.CSS_SELECTOR, \"div.topic-post\"))\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "\n",
    "    print(f\"Loaded {posts_loaded} posts in topic\")\n",
    "\n",
    "def scrape_topic_to_text(driver, title, topic_url, output_dir=\"output\"):\n",
    "    try:\n",
    "        driver.get(topic_url)\n",
    "        WebDriverWait(driver, 15).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, \"div.cooked\"))\n",
    "        )\n",
    "        scroll_to_load_posts(driver)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load topic {topic_url}: {e}\")\n",
    "        return\n",
    "\n",
    "    # Get page title\n",
    "    try:\n",
    "        page_title = driver.find_element(By.CSS_SELECTOR, \"h1.title\").text.strip()\n",
    "    except:\n",
    "        page_title = title\n",
    "\n",
    "    posts = []\n",
    "    post_containers = driver.find_elements(By.CSS_SELECTOR, \"div.topic-post\")\n",
    "    if not post_containers:\n",
    "        post_containers = driver.find_elements(By.CSS_SELECTOR, \"div.cooked\")\n",
    "\n",
    "    for container in post_containers:\n",
    "        author = None\n",
    "        timestamp = None\n",
    "        content = None\n",
    "\n",
    "        try:\n",
    "            author = container.find_element(By.CSS_SELECTOR, \".creator a.username\").text.strip()\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            time_el = container.find_element(By.CSS_SELECTOR, \"time\")\n",
    "            timestamp = time_el.get_attribute(\"datetime\")\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            content = container.find_element(By.CSS_SELECTOR, \"div.cooked\").text.strip()\n",
    "        except:\n",
    "            content = container.text.strip()\n",
    "\n",
    "        posts.append({\n",
    "            \"author\": author,\n",
    "            \"timestamp\": timestamp,\n",
    "            \"content\": content\n",
    "        })\n",
    "\n",
    "    # Save to .txt\n",
    "    safe_title = \"\".join(c if c.isalnum() or c in \" _-\" else \"_\" for c in page_title)[:80]\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    filepath = os.path.join(output_dir, f\"{safe_title}.txt\")\n",
    "\n",
    "    with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"Topic: {page_title}\\nURL: {topic_url}\\n\\n\")\n",
    "        for idx, post in enumerate(posts, start=1):\n",
    "            f.write(f\"--- Post {idx} ---\\n\")\n",
    "            f.write(f\"Author: {post.get('author')}\\n\")\n",
    "            f.write(f\"Timestamp: {post.get('timestamp')}\\n\")\n",
    "            f.write(\"Content:\\n\")\n",
    "            f.write(post.get(\"content\", \"\") + \"\\n\\n\")\n",
    "\n",
    "    print(f\"Wrote {len(posts)} posts to {filepath}\")\n",
    "\n",
    "def scrape_all_latest(target_topics=200):\n",
    "    driver = init_driver(headless=False)  # Set headless=True to run without UI\n",
    "    try:\n",
    "        topic_links = scroll_and_collect_topics(driver, target_count=target_topics)\n",
    "        for idx, (title, href) in enumerate(topic_links, start=1):\n",
    "            print(f\"Scraping topic {idx}/{len(topic_links)}: {title}\")\n",
    "            scrape_topic_to_text(driver, title, href)\n",
    "            time.sleep(1)  # polite delay\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scrape_all_latest(target_topics=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ece8c89a-8101-4dd3-808e-995c7be716bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 250 topics\n",
      "ðŸ” Resuming from: My PVCs are causing crippling anxiety and panic attacks. Help!!!!\n",
      "\n",
      "ðŸ“„ Scraping topic 1/49: My PVCs are causing crippling anxiety and panic attacks. Help!!!!\n",
      "Loaded 1 posts in topic\n",
      "Wrote 1 posts to output\\My PVCs are causing crippling anxiety and panic attacks_ Help____.txt\n",
      "\n",
      "ðŸ“„ Scraping topic 2/49: PVCs after every 2 beats, all day\n",
      "Loaded 10 posts in topic\n",
      "Wrote 10 posts to output\\PVCs after every 2 beats_ all day.txt\n",
      "â© Skipping (already saved): Pvcs\n",
      "\n",
      "ðŸ“„ Scraping topic 4/49: Heart\n",
      "Loaded 2 posts in topic\n",
      "Wrote 2 posts to output\\Heart.txt\n",
      "\n",
      "ðŸ“„ Scraping topic 5/49: Arrhythmogenic cardiomyopathy\n",
      "Loaded 1 posts in topic\n",
      "Wrote 1 posts to output\\Arrhythmogenic cardiomyopathy.txt\n",
      "\n",
      "ðŸ“„ Scraping topic 6/49: From my first SVT episode to my ablation\n",
      "Loaded 3 posts in topic\n",
      "Wrote 3 posts to output\\From my first SVT episode to my ablation.txt\n",
      "\n",
      "ðŸ“„ Scraping topic 7/49: Is this normal after ablation\n",
      "Loaded 1 posts in topic\n",
      "Wrote 1 posts to output\\Is this normal after ablation.txt\n",
      "\n",
      "ðŸ“„ Scraping topic 8/49: Alternative Drugs for Angina\n",
      "Loaded 2 posts in topic\n",
      "Wrote 2 posts to output\\Alternative Drugs for Angina.txt\n",
      "\n",
      "ðŸ“„ Scraping topic 9/49: Vein problems-donâ€™t know what\n",
      "Loaded 1 posts in topic\n",
      "Wrote 1 posts to output\\Vein problems-don_t know what.txt\n",
      "\n",
      "ðŸ“„ Scraping topic 10/49: Hypertension - drank alcohol\n",
      "Loaded 1 posts in topic\n",
      "Wrote 1 posts to output\\Hypertension - drank alcohol.txt\n",
      "\n",
      "ðŸ“„ Scraping topic 11/49: I have 2 sets of ECGâ€™s with PVCâ€™s on them\n",
      "Loaded 1 posts in topic\n",
      "Wrote 1 posts to output\\I have 2 sets of ECG_s with PVC_s on them.txt\n",
      "\n",
      "ðŸ“„ Scraping topic 12/49: 15000-25000 PVCs a day for weeks now\n",
      "Loaded 2 posts in topic\n",
      "Wrote 2 posts to output\\15000-25000 PVCs a day for weeks now.txt\n",
      "\n",
      "ðŸ“„ Scraping topic 13/49: AF?\n",
      "Loaded 7 posts in topic\n",
      "Wrote 7 posts to output\\AF_.txt\n",
      "â© Skipping (already saved): Eptopic beats\n",
      "\n",
      "ðŸ“„ Scraping topic 15/49: Can you feel irregular hearbeats under an electronic BP monitor cuff?\n",
      "Loaded 2 posts in topic\n",
      "Wrote 2 posts to output\\Can you feel irregular hearbeats under an electronic BP monitor cuff_.txt\n",
      "\n",
      "ðŸ“„ Scraping topic 16/49: Anyone get ectopics(Like extra beats) when stressed or anxious that cause quick pain?\n",
      "Loaded 2 posts in topic\n",
      "Wrote 2 posts to output\\Anyone get ectopics_Like extra beats_ when stressed or anxious that cause quick .txt\n",
      "\n",
      "ðŸ“„ Scraping topic 17/49: Good experiences of flecainide?\n",
      "Loaded 12 posts in topic\n",
      "Wrote 12 posts to output\\Good experiences of flecainide_.txt\n",
      "\n",
      "ðŸ“„ Scraping topic 18/49: I got pains on the chest. My doctor says it might be Syndrom of Tietze. But I need more perspectives\n",
      "Loaded 1 posts in topic\n",
      "Wrote 1 posts to output\\I got pains on the chest_ My doctor says it might be Syndrom of Tietze_ But I ne.txt\n",
      "\n",
      "ðŸ“„ Scraping topic 19/49: Uk health insurance\n",
      "Loaded 9 posts in topic\n",
      "Wrote 9 posts to output\\Uk health insurance.txt\n",
      "\n",
      "ðŸ“„ Scraping topic 20/49: Had my ablation today!\n",
      "Loaded 5 posts in topic\n",
      "Wrote 5 posts to output\\Had my ablation today_.txt\n",
      "\n",
      "ðŸ“„ Scraping topic 21/49: I have flutters n my heart Iâ€™ve had ECG\n",
      "Loaded 1 posts in topic\n",
      "Wrote 1 posts to output\\I have flutters n my heart I_ve had ECG.txt\n",
      "\n",
      "ðŸ“„ Scraping topic 22/49: Dofetilide for Atrial Fibrillation - anyone with experience?\n",
      "Loaded 1 posts in topic\n",
      "Wrote 1 posts to output\\Dofetilide for Atrial Fibrillation - anyone with experience_.txt\n",
      "\n",
      "ðŸ“„ Scraping topic 23/49: Ectopic Beats Linked to Digestive / Indigestion Issues?\n",
      "Loaded 2 posts in topic\n",
      "Wrote 2 posts to output\\Ectopic Beats Linked to Digestive _ Indigestion Issues_.txt\n",
      "\n",
      "ðŸ“„ Scraping topic 24/49: Exercise with Afib\n",
      "Loaded 3 posts in topic\n",
      "Wrote 3 posts to output\\Exercise with Afib.txt\n",
      "\n",
      "ðŸ“„ Scraping topic 25/49: Loop monitor and svt\n",
      "Loaded 2 posts in topic\n",
      "Wrote 2 posts to output\\Loop monitor and svt.txt\n",
      "\n",
      "ðŸ“„ Scraping topic 26/49: Peripheral arterial disease - outlook\n",
      "Loaded 1 posts in topic\n",
      "Wrote 1 posts to output\\Peripheral arterial disease - outlook.txt\n",
      "\n",
      "ðŸ“„ Scraping topic 27/49: nausea and vomiting with SVT\n",
      "Loaded 13 posts in topic\n",
      "Wrote 13 posts to output\\nausea and vomiting with SVT.txt\n",
      "\n",
      "ðŸ“„ Scraping topic 28/49: I woke up with my first afib attack.\n",
      "Loaded 2 posts in topic\n",
      "Wrote 2 posts to output\\I woke up with my first afib attack_.txt\n",
      "\n",
      "ðŸ“„ Scraping topic 29/49: Ectopics & Low Heart Rate\n",
      "Loaded 3 posts in topic\n",
      "Wrote 3 posts to output\\Ectopics _ Low Heart Rate.txt\n",
      "\n",
      "ðŸ“„ Scraping topic 30/49: Ectopics\n",
      "Loaded 5 posts in topic\n",
      "Wrote 5 posts to output\\Ectopics.txt\n",
      "\n",
      "ðŸ“„ Scraping topic 31/49: Sigh. Another PVC topic.\n",
      "Loaded 3 posts in topic\n",
      "Wrote 3 posts to output\\Sigh_ Another PVC topic_.txt\n",
      "\n",
      "ðŸ“„ Scraping topic 32/49: Monitor goes back tomorrow nocturnal thuds trigeminy\n",
      "Loaded 2 posts in topic\n",
      "Wrote 2 posts to output\\Monitor goes back tomorrow nocturnal thuds trigeminy.txt\n",
      "\n",
      "ðŸ“„ Scraping topic 33/49: Diagnosed with svt and its controlling my life. Should I do the ablation surgery?\n",
      "Loaded 18 posts in topic\n",
      "Wrote 18 posts to output\\Diagnosed with svt and its controlling my life_ Should I do the ablation surgery.txt\n",
      "\n",
      "ðŸ“„ Scraping topic 34/49: Is 25,000 pacs a day\n",
      "Loaded 3 posts in topic\n",
      "Wrote 3 posts to output\\Is 25_000 pacs a day.txt\n",
      "\n",
      "ðŸ“„ Scraping topic 35/49: AFIB caused by Ibuprofen\n",
      "Loaded 13 posts in topic\n",
      "Wrote 13 posts to output\\AFIB caused by Ibuprofen.txt\n",
      "\n",
      "ðŸ“„ Scraping topic 36/49: They canâ€™t find anything, but itâ€™s getting worse.\n",
      "Loaded 14 posts in topic\n",
      "Wrote 14 posts to output\\They can_t find anything_ but it_s getting worse_.txt\n",
      "\n",
      "ðŸ“„ Scraping topic 37/49: What vein comes over/around the knee cap\n",
      "Loaded 1 posts in topic\n",
      "Wrote 1 posts to output\\What vein comes over_around the knee cap.txt\n",
      "\n",
      "ðŸ“„ Scraping topic 38/49: Frequent and Sharp Blood Pressure Fluctuations\n",
      "Loaded 4 posts in topic\n",
      "Wrote 4 posts to output\\Frequent and Sharp Blood Pressure Fluctuations.txt\n",
      "\n",
      "ðŸ“„ Scraping topic 39/49: Valve replacement and Post perfusion syndrome\n",
      "Loaded 1 posts in topic\n",
      "Wrote 1 posts to output\\Valve replacement and Post perfusion syndrome.txt\n",
      "\n",
      "ðŸ“„ Scraping topic 40/49: Audible External Heartbeat\n",
      "Loaded 1 posts in topic\n",
      "Wrote 1 posts to output\\Audible External Heartbeat.txt\n",
      "\n",
      "ðŸ“„ Scraping topic 41/49: Atrial fib and cardioversion\n",
      "Loaded 1 posts in topic\n",
      "Wrote 1 posts to output\\Atrial fib and cardioversion.txt\n",
      "\n",
      "ðŸ“„ Scraping topic 42/49: Need some advice on these episodes\n",
      "Loaded 1 posts in topic\n",
      "Wrote 1 posts to output\\Need some advice on these episodes.txt\n",
      "\n",
      "ðŸ“„ Scraping topic 43/49: Chest Pains! Suggestions please\n",
      "Loaded 1 posts in topic\n",
      "Wrote 1 posts to output\\Chest Pains_ Suggestions please.txt\n",
      "\n",
      "ðŸ“„ Scraping topic 44/49: New right bundle branch block\n",
      "Loaded 2 posts in topic\n",
      "Wrote 2 posts to output\\New right bundle branch block.txt\n",
      "\n",
      "ðŸ“„ Scraping topic 45/49: Right Bundle Branch Block\n",
      "Loaded 1 posts in topic\n",
      "Wrote 1 posts to output\\Right Bundle Branch Block.txt\n",
      "\n",
      "ðŸ“„ Scraping topic 46/49: Losartan\n",
      "Loaded 1 posts in topic\n",
      "Wrote 1 posts to output\\Losartan.txt\n",
      "\n",
      "ðŸ“„ Scraping topic 47/49: PVC s and light headed ness.\n",
      "Loaded 1 posts in topic\n",
      "Wrote 1 posts to output\\PVC s and light headed ness_.txt\n",
      "\n",
      "ðŸ“„ Scraping topic 48/49: Elevated Blood Pressure With Meds\n",
      "Loaded 1 posts in topic\n",
      "Wrote 1 posts to output\\Elevated Blood Pressure With Meds.txt\n",
      "\n",
      "ðŸ“„ Scraping topic 49/49: samsung watch 4 and AFÄ°B\n",
      "Loaded 1 posts in topic\n",
      "Wrote 1 posts to output\\samsung watch 4 and AFÄ°B.txt\n",
      "\n",
      "ðŸŽ‰ Done! Resumed scraping finished.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "def init_driver(headless=True):\n",
    "    options = webdriver.ChromeOptions()\n",
    "    if headless:\n",
    "        options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument(\"--window-size=1920,1080\")\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    driver.implicitly_wait(5)\n",
    "    return driver\n",
    "\n",
    "def scroll_and_collect_topics(driver, target_count=200, pause_time=2):\n",
    "    \"\"\"Scrolls the latest page to collect topic links.\"\"\"\n",
    "    driver.get(\"https://community.patient.info/c/heart-health-and-blood-vessels/18\")\n",
    "    topic_links = []\n",
    "\n",
    "    while len(topic_links) < target_count:\n",
    "        # Scroll to bottom\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(pause_time)\n",
    "\n",
    "        # Collect links\n",
    "        topics = driver.find_elements(By.CSS_SELECTOR, \"a.title.raw-topic-link\")\n",
    "        for topic in topics:\n",
    "            href = topic.get_attribute(\"href\")\n",
    "            title = topic.text.strip()\n",
    "            if href and (title, href) not in topic_links:\n",
    "                topic_links.append((title, href))\n",
    "\n",
    "    print(f\"Collected {len(topic_links[:target_count])} topics\")\n",
    "    return topic_links[:target_count]\n",
    "\n",
    "def scroll_to_load_posts(driver, target_count=9999, pause_time=1):\n",
    "    \"\"\"Scrolls inside a topic to load all posts.\"\"\"\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    posts_loaded = len(driver.find_elements(By.CSS_SELECTOR, \"div.topic-post\"))\n",
    "\n",
    "    while posts_loaded < target_count:\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(pause_time)\n",
    "        posts_loaded = len(driver.find_elements(By.CSS_SELECTOR, \"div.topic-post\"))\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "\n",
    "    print(f\"Loaded {posts_loaded} posts in topic\")\n",
    "\n",
    "def scrape_topic_to_text(driver, title, topic_url, output_dir=\"output\"):\n",
    "    try:\n",
    "        driver.get(topic_url)\n",
    "        WebDriverWait(driver, 15).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, \"div.cooked\"))\n",
    "        )\n",
    "        scroll_to_load_posts(driver)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load topic {topic_url}: {e}\")\n",
    "        return\n",
    "\n",
    "    # Get page title\n",
    "    try:\n",
    "        page_title = driver.find_element(By.CSS_SELECTOR, \"h1.title\").text.strip()\n",
    "    except:\n",
    "        page_title = title\n",
    "\n",
    "    posts = []\n",
    "    post_containers = driver.find_elements(By.CSS_SELECTOR, \"div.topic-post\")\n",
    "    if not post_containers:\n",
    "        post_containers = driver.find_elements(By.CSS_SELECTOR, \"div.cooked\")\n",
    "\n",
    "    for container in post_containers:\n",
    "        author = None\n",
    "        timestamp = None\n",
    "        content = None\n",
    "\n",
    "        try:\n",
    "            author = container.find_element(By.CSS_SELECTOR, \".creator a.username\").text.strip()\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            time_el = container.find_element(By.CSS_SELECTOR, \"time\")\n",
    "            timestamp = time_el.get_attribute(\"datetime\")\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            content = container.find_element(By.CSS_SELECTOR, \"div.cooked\").text.strip()\n",
    "        except:\n",
    "            content = container.text.strip()\n",
    "\n",
    "        posts.append({\n",
    "            \"author\": author,\n",
    "            \"timestamp\": timestamp,\n",
    "            \"content\": content\n",
    "        })\n",
    "\n",
    "    # Save to .txt\n",
    "    safe_title = \"\".join(c if c.isalnum() or c in \" _-\" else \"_\" for c in page_title)[:80]\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    filepath = os.path.join(output_dir, f\"{safe_title}.txt\")\n",
    "\n",
    "    with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"Topic: {page_title}\\nURL: {topic_url}\\n\\n\")\n",
    "        for idx, post in enumerate(posts, start=1):\n",
    "            f.write(f\"--- Post {idx} ---\\n\")\n",
    "            f.write(f\"Author: {post.get('author')}\\n\")\n",
    "            f.write(f\"Timestamp: {post.get('timestamp')}\\n\")\n",
    "            f.write(\"Content:\\n\")\n",
    "            f.write(post.get(\"content\", \"\") + \"\\n\\n\")\n",
    "\n",
    "    print(f\"Wrote {len(posts)} posts to {filepath}\")\n",
    "\n",
    "def scrape_all_latest(target_topics=200, headless=False, start_from=None):\n",
    "    driver = init_driver(headless=headless)\n",
    "    try:\n",
    "        topic_links = scroll_and_collect_topics(driver, target_count=target_topics)\n",
    "\n",
    "        # If you know where to start, find its index\n",
    "        if start_from:\n",
    "            start_idx = 0\n",
    "            for i, (title, _) in enumerate(topic_links):\n",
    "                if start_from.lower() in title.lower():\n",
    "                    start_idx = i\n",
    "                    break\n",
    "            topic_links = topic_links[start_idx:]\n",
    "            print(f\"ðŸ” Resuming from: {topic_links[0][0]}\")\n",
    "        else:\n",
    "            print(\"ðŸ” Starting from the top.\")\n",
    "\n",
    "        for idx, (title, href) in enumerate(topic_links, start=1):\n",
    "            # Skip if file already exists (useful for partial reruns)\n",
    "            safe_title = \"\".join(c if c.isalnum() or c in \" _-\" else \"_\" for c in title)[:100]\n",
    "            filepath = os.path.join(\"output\", f\"{safe_title}.txt\")\n",
    "            if os.path.exists(filepath):\n",
    "                print(f\"â© Skipping (already saved): {title}\")\n",
    "                continue\n",
    "\n",
    "            print(f\"\\nðŸ“„ Scraping topic {idx}/{len(topic_links)}: {title}\")\n",
    "            scrape_topic_to_text(driver, title, href)\n",
    "            time.sleep(1.5)\n",
    "    finally:\n",
    "        driver.quit()\n",
    "        print(\"\\nðŸŽ‰ Done! Resumed scraping finished.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scrape_all_latest(\n",
    "        target_topics=250,\n",
    "        headless=False,\n",
    "        start_from=\"My PVCs are causing crippling anxiety and panic attacks. Help!!!!\"\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667e05f2-9621-46a7-9b66-2146a760f25d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7368a2a-0073-44ec-bad7-ba6362934b06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
